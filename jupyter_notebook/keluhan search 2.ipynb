{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import tweepy\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(\"CkhGa0z1CKMlK7GY56Zxh0250\", \n",
    "    \"Iq1o1IwRzuFw6xCandQbGUX1gzx6GezPD7ek1ptbcw46eni3LB\")\n",
    "auth.set_access_token(\"492948056-n4H0mlG3efeuJnDKkkoyeW9BWbl7kBrI95Dmlup1\", \n",
    "    \"jMSNiTz1mo764Xnz1x8cEAuu2CQRIhRbstAgRiD5WY7cP\")\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koneksi Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  '''\n",
    "  DESCRIPTION:\n",
    "  This function to clean text \n",
    "  INPUT: \n",
    "  text: string\n",
    "  OUTPUT: \n",
    "  text: string after clean it\n",
    "  ''' \n",
    "  text = text.lower() # convert letters to lower case\n",
    "  text = re.sub(\"@[A-Za-z0-9_]+\",\" \", text) #r emove mention\n",
    "  text = re.sub(\"#[A-Za-z0-9_]+\",\" \", text) # remove hashtag\n",
    "  text = re.sub(r\"\\\\n\",\" \",text) # remove \\n or enter\n",
    "  text = re.sub(\"[^a-zA-Z0]\", \" \", text) # remove non-letters\n",
    "  # text = re.sub(r'\\d+', '', text) # remove number\n",
    "  text = re.sub(r'http\\S+', '', text) # remove links\n",
    "  text = re.sub(r\"www.\\S+\", \" \", text) # remove link\n",
    "  text = re.sub(\"rt\",\" \",text) # remove RT\n",
    "  text = text.translate(str.maketrans('','', string.punctuation)) # remove punctuation\n",
    "  text = re.sub(' +', ' ',text) # remove extra space\n",
    "  text = text.strip() # remove whitespaces\n",
    "\n",
    "  # remove stopwords\n",
    "  stpwds_id = list(set(stopwords.words('indonesian')))\n",
    "  stpwds_id.append('oh')\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  text = ' '.join([word for word in tokens if word not in stpwds_id])\n",
    "\n",
    "  # Stemming\n",
    "  factory = StemmerFactory()\n",
    "  stemmer = factory.create_stemmer()\n",
    "  text = stemmer.stem(text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://backend-auto-reply-fp.herokuapp.com/v1/models/auto_reply:predict'\n",
    "def get_predict(series, URL_Serving):\n",
    "    X=[]\n",
    "    for con in series.tolist():\n",
    "        X.append([con])\n",
    "\n",
    "    input_data_json = json.dumps({\n",
    "        \"signature_name\": \"serving_default\",\n",
    "        \"instances\": X,\n",
    "        })\n",
    "        \n",
    "    r=requests.post(URL_Serving, input_data_json)\n",
    "    result=r.json()\n",
    "    result= np.argmax(result['predictions'], axis=1)\n",
    "    return np.where(result==0,'Neutral', np.where(result==1, 'Negative', 'Positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tweet(query, api, cursor, result_type='recent', response=True):\n",
    "    max_id=cursor.execute('SELECT MAX(ID) FROM tweet').fetchall()[0][0]\n",
    "    data = api.search_tweets(q=query, \n",
    "                        result_type=result_type,\n",
    "                        count=100, \n",
    "                        tweet_mode ='extended', \n",
    "                        since_id= max_id)\n",
    "    data= pd.DataFrame([[s.id, s.full_text.replace('\\n','').replace('\\r',''), s.user.screen_name, s.user.id_str, s.user.followers_count, s.retweet_count, s.favorite_count, s.created_at] for s in data], columns=('ID', 'Texts', 'UserName','UserID', \"UserFollowerCount\", 'RetweetCount', 'Likes', \"CreatedAt\"))\n",
    "    if data.shape[0]>0: \n",
    "        data.iloc[:][\"clean_text\"]=data.iloc[:]['Texts'].apply(clean_text)\n",
    "        data['sentimen']=get_predict(data['Texts'], URL)\n",
    "        if response:\n",
    "            for id in data[(data.sentimen==\"Negative\")&(data.UserID!='492948056')].ID:\n",
    "                name=data[data.ID==id][\"UserName\"].values[0]\n",
    "                user_id=data[data.ID==id][\"UserID\"].values[0]\n",
    "                status=\"Halo kak @\"+name+\". maaf atas ketidaknyamanannya. Untuk informasi lebih lanjut, cek dm ya\"\n",
    "                pesan=\"Maaf kak atas kendala yang dihadapi. Bisa diceritakan lebih detail terkait masalah yang kakak hadapi?\"\n",
    "                try:\n",
    "                    api.update_status(status, in_reply_to_status_id=id)\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    print(\"gagal balas\")\n",
    "                try:\n",
    "                    api.send_direct_message(user_id,pesan)\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    print(\"gagal dm\")\n",
    "    return data[data.UserID!='492948056']\n",
    "    # else: return \"tidak ada tweet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_connection(\"twit copy.db\")\n",
    "cursor=con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek limit request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 180, 'remaining': 173, 'reset': 1659312965}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 Forbidden\n",
      "187 - Status is a duplicate.\n",
      "gagal balas\n",
      "Data baru sejumlah 3\n",
      "Data baru sejumlah 0\n",
      "Data baru sejumlah 0\n",
      "Data baru sejumlah 0\n",
      "Data baru sejumlah 0\n",
      "Data baru sejumlah 0\n",
      "Data baru sejumlah 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wahyu\\github-classroom\\H8-Assignments-Bay\\final\\p2---final-project-ftds-012-group-003\\keluhan search 2.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wahyu/github-classroom/H8-Assignments-Bay/final/p2---final-project-ftds-012-group-003/keluhan%20search%202.ipynb#ch0000017?line=2'>3</a>\u001b[0m new_data\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m'\u001b[39m, con\u001b[39m=\u001b[39mcon, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti\u001b[39m\u001b[39m'\u001b[39m, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wahyu/github-classroom/H8-Assignments-Bay/final/p2---final-project-ftds-012-group-003/keluhan%20search%202.ipynb#ch0000017?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData baru sejumlah \u001b[39m\u001b[39m{\u001b[39;00mnew_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wahyu/github-classroom/H8-Assignments-Bay/final/p2---final-project-ftds-012-group-003/keluhan%20search%202.ipynb#ch0000017?line=4'>5</a>\u001b[0m sleep(\u001b[39m6\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    new_data=get_tweet('\"bank t0yip\" OR t0yip',api, cursor)\n",
    "    new_data.to_sql('tweet', con=con, method='multi', if_exists='append', index=False)\n",
    "    print(f'Data baru sejumlah {new_data.shape[0]}')\n",
    "    sleep(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9048cbf2aa5c5605910089358556f0bf71534cc3a4ea7c995cde1c5f16d23cd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
